{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481856b3",
   "metadata": {},
   "source": [
    "# **NYC TAXI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40071c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFull = pd.read_csv('../data/train.csv')\n",
    "testFull = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a4111",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "138d31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datetime_features(data):\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "\n",
    "    #Extract time features\n",
    "    data['pickup_hour'] = data['pickup_datetime'].dt.hour\n",
    "    data['pickup_day'] = data['pickup_datetime'].dt.day\n",
    "    data['pickup_dayofweek'] = data['pickup_datetime'].dt.dayofweek\n",
    "    data['pickup_month'] = data['pickup_datetime'].dt.month\n",
    "    \n",
    "    #Pretty cool thing here cuz you map the time and day to a circle so the model understands\n",
    "    #That 12AM (hour 0) and 11PM (23) are not far apart for example\n",
    "    data['pickup_hour_sin'] = np.sin(2 * np.pi * data['pickup_hour'] / 24)\n",
    "    data['pickup_hour_cos'] = np.cos(2 * np.pi * data['pickup_hour'] / 24)\n",
    "    data['pickup_dayofweek_sin'] = np.sin(2 * np.pi * data['pickup_dayofweek'] / 7)\n",
    "    data['pickup_dayofweek_cos'] = np.cos(2 * np.pi * data['pickup_dayofweek'] / 7)\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_geo_features(data):\n",
    "    #Calculate the shortest distance through 2 points\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        radius = 6371  #Earth radius in km\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        return radius * c\n",
    "\n",
    "    #Very rough estimation of how you would actually drive to the point\n",
    "    #since we can't drive through buildings\n",
    "    def street_distance(lat1, lon1, lat2, lon2):\n",
    "        lat_dist = haversine_distance(lat1, lon1, lat2, lon1)\n",
    "        lon_dist = haversine_distance(lat1, lon1, lat1, lon2)\n",
    "        return lat_dist + lon_dist\n",
    "    \n",
    "    data['haversine_distance'] = haversine_distance(data['pickup_latitude'], data['pickup_longitude'], data['dropoff_latitude'], data['dropoff_longitude'])\n",
    "    data['street_distance'] = street_distance(data['pickup_latitude'], data['pickup_longitude'], data['dropoff_latitude'], data['dropoff_longitude'])\n",
    "    \n",
    "    #Taxi speeds depend on how far they are from the city's \"center\"\n",
    "    #I chose the East Village in Manhattan but thats an arbitrary choice\n",
    "    nyc_center = (40.72680913695419, -73.98296948105471) #East Village 131 Ave A\n",
    "\n",
    "    data['dropoff_distance_from_center'] = haversine_distance(data['dropoff_latitude'], data['dropoff_longitude'], nyc_center[0], nyc_center[1])\n",
    "    data['pickup_distance_from_center'] = haversine_distance(data['pickup_latitude'], data['pickup_longitude'], nyc_center[0], nyc_center[1])\n",
    "\n",
    "    return data\n",
    "\n",
    "trainFull = create_datetime_features(trainFull)\n",
    "trainFull = create_geo_features(trainFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45651446",
   "metadata": {},
   "source": [
    "#**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "507d4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select features for the model\n",
    "feature_columns = [\n",
    "    # Geographic features\n",
    "    'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude',\n",
    "    'haversine_distance', 'street_distance',\n",
    "    'pickup_distance_from_center', 'dropoff_distance_from_center',\n",
    "    \n",
    "    # Time features\n",
    "    'pickup_hour_sin', 'pickup_hour_cos',\n",
    "    'pickup_dayofweek_sin', 'pickup_dayofweek_cos',\n",
    "    'pickup_day', 'pickup_month',\n",
    "]\n",
    "\n",
    "X = trainFull[feature_columns]\n",
    "y = trainFull['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a0f38",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning: Train models with different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8188cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39)\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 3, 5, 7, 10],\n",
    "    'min_samples_leaf': [1, 5, 10, 20],\n",
    "    'l2_regularization': [0, 0.1, 1, 10],\n",
    "    'max_leaf_nodes': [15, 31, 63, 127],\n",
    "}\n",
    "\n",
    "base_model = HistGradientBoostingRegressor(\n",
    "    random_state=39,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=hyperparams,\n",
    "    n_iter=100,\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "#fit the random search\n",
    "print(\"Currently tuning hyperparameters. Sit tight...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {random_search.best_score_}\")\n",
    "\n",
    "#Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "#Evaluate on validation set\n",
    "val_predictions = best_model.predict(X_val)\n",
    "val_score = accuracy_score(y_val, val_predictions)\n",
    "print(f\"\\nAccuracy score: {val_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
